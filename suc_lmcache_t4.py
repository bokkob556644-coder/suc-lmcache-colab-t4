# -*- coding: utf-8 -*-
"""suc_LMCache_t4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YFuR0ZHRpDHuPNMvO7m7gCNTEMFNCppz
"""



















"""الحل النهائي والصحيح
الحل هو التأكد من أنك تبني المكتبة من الكود المصدري بشكل صحيح. البيئة لديك الآن في حالة غير مستقرة، لذا يجب أن نبدأ من جديد بخطوات نظيفة لضمان النجاح.
يرجى تنفيذ هذه الخطوات بالترتيب في دفتر الملاحظات الخاص بك:
الخطوة 1: التنظيف (مهم جدًا)
أولاً، قم بإزالة أي تثبيتات قديمة أو تالفة لكلتا المكتبتين.
code
Bash
!pip uninstall -y vllm lmcache
الخطوة 2: الانتقال إلى المجلد الصحيح
تأكد من أنك تعمل داخل مجلد LMCache الذي قمت باستنساخه.
code
Bash
%cd /content/LMCache
الخطوة 3: تثبيت أدوات البناء (خطوة وقائية)
هذا يضمن وجود كل ما تحتاجه لعملية الترجمة.
code
Bash
!pip install -r requirements/build.txt
الخطوة 4: بناء وتثبيت LMCache من المصدر (الخطوة الحاسمة)
هذا هو الأمر الذي يقوم بترجمة كود C++ وإنشاء ملف c_ops المفقود.
code
Bash
!pip install -e .
-e . تعني التثبيت في "وضع قابل للتعديل" من المجلد الحالي، وهذا هو ما يقوم بتشغيل عملية البناء.
الخطوة 5: تثبيت vllm
الآن بعد أن تم تثبيت LMCache بشكل صحيح، قم بتثبيت vllm.
code
Bash
!pip install vllm
الخطوة 6: إعادة تشغيل الكود الخاص بك
بعد اكتمال الخطوات أعلاه بنجاح، أعد تشغيل خلية البايثون التي تحتوي على الكود الخاص بك. يجب أن تعمل الآن دون أي مشاكل.
code
Python
from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# يجب أن يعمل هذا الكود الآن
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

llm = LLM(model="Qwen/Qwen3-8B",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)

print("LLM with LMCache initialized successfully!")
28.0s
Use Arrow Up and Arrow Down to select a turn, Enter to jump to it, and Escape to return to the chat.

"""





!git clone https://github.com/LMCache/LMCache.git

!pip uninstall -y vllm lmcache

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache

!pip install -r requirements/build.txt

!pip install -e .

!!pip install vllm



from PIL import Image
from IPython.display import display

# Replace 'path/to/your/image.jpg' with the actual path to your image file
image_path = '/content/lmcache.png'

try:
    # Open the image file
    img = Image.open(image_path)

    # Display the image in the notebook output
    display(img)

except FileNotFoundError:
    print(f"Error: Image file not found at '{image_path}'")
except Exception as e:
    print(f"An error occurred: {e}")

from PIL import Image
from IPython.display import display

# Replace 'path/to/your/image.jpg' with the actual path to your image file
image_path = '/content/1.png'

try:
    # Open the image file
    img = Image.open(image_path)

    # Display the image in the notebook output
    display(img)

except FileNotFoundError:
    print(f"Error: Image file not found at '{image_path}'")
except Exception as e:
    print(f"An error occurred: {e}")

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# Configure KV cache transfer to use LMCache
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

# Initialize LLM with LMCache configuration
# Adjust gpu_memory_utilization based on your GPU memory
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)



# Create example prompts with shared prefix
shared_prompt = "Hello, how are you?" * 1000
prompts = [
    shared_prompt + "python is",
]

# Define sampling parameters
sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=128)

# Run inference
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    generated_text = output.outputs[0].text
    print(f"Generated text: {generated_text!r}")













!git clone https://github.com/LMCache/LMCache.git

# Commented out IPython magic to ensure Python compatibility.
# %cd LMCache

uv venv --python 3.12
source .venv/bin/activate
# LMCache wheels are built with the latest version of torch.
# vllm is required for LMCache to work; you may also choose to install it separately.

!uv pip install lmcache vllm

import lmcache
from importlib.metadata import version
print(version("lmcache"))

0.3.4.dev61 # should be the latest pre-release version you installed

!pip list

/content/LMCache/lmcache/integration/vllm/vllm_v1_adapter.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/
from lmcache.integration.vllm.vllm_v1_adapter import LMCacheConnectorV1Impl

!uv pip install vllm==0.4.0 # Attempt to install a specific version of vllm

from vllm.config import KVTransferConfig
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

vllm serve "YOUR_MODEL" \
    --kv-transfer-config \
    '{"kv_connector":"LMCacheConnectorV1", "kv_role":"kv_both"}'

!python3 -c "import vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector"

!vllm serve "facebook/opt-125m" \
    --kv-transfer-config \
    '{"kv_connector":"LMCacheConnectorV1Dynamic","kv_role":"kv_both","kv_connector_module_path":"lmcache.integration.vllm.lmcache_connector_v1"}'

!wget https://github.com/LMCache/LMCache/releases/download/v0.3.8/lmcache-0.3.8-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl

!pip install lmcache-0.3.8-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl

!python3 -c "import vllm.distributed.kv_transfer.kv_connector.v1.lmcache_connector"

!pip install vllm==0.2.7

!pip install -e .

!pip install -r /content/LMCache/requirements/cuda.txt

!pip install -r /content/LMCache/requirements/build.txt

!pip install -e .

!pip install LMCache vllm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/examples/agents
!pip install -r /content/LMCache/examples/agents/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/examples/agents
!python prefix_analysis.py -h

# Commented out IPython magic to ensure Python compatibility.
#!/usr/bin/env python3
# SPDX-License-Identifier: Apache-2.0

# Standard
from collections import OrderedDict
from typing import List, Optional, Tuple, Union
import argparse
import json

# Third Party
from tqdm import tqdm
from transformers import AutoTokenizer
import matplotlib.pyplot as plt

# Constants
DEFAULT_TOKENIZER = "meta-llama/Llama-3.1-8B"
DEFAULT_TOKENS_PER_GB = 8200  # Default for Llama-3.1; More details here: https://docs.lmcache.ai/getting_started/kv_cache_calculator.html
DEFAULT_POOL_SIZES_GB: List[Union[int, float, str]] = [
    1,
    2,
    4,
    8,
    16,
    32,
    50,
    100,
    200,
    500,
    "unlimited",
]


class LRUTokenPool:
    """
    Token pool with LRU eviction policy based on token count limit.
    For request i (1-indexed):
    y[i] = y[i-1] + (len(tokens[i]) - max_shared_prefix(tokens[i], any previous))
    """

    def __init__(self, max_tokens: float) -> None:
        self.max_tokens = max_tokens
        self.current_tokens = 0
        self.requests: OrderedDict[int, List[int]] = OrderedDict()

    def longest_prefix_len(self, tokens: List[int]) -> Tuple[int, int]:
        """
        Find longest prefix match and update LRU ordering.

        Returns:
            Tuple of (prefix_length, matching_request_id)
        """
        best_len = 0
        best_id = -1

        for req_id, req_tokens in self.requests.items():
            common_len = 0
            for i in range(min(len(tokens), len(req_tokens))):
                if tokens[i] == req_tokens[i]:
                    common_len += 1
                else:
                    break

            if common_len > best_len:
                best_len = common_len
                best_id = req_id

        # Update LRU ordering
        if best_id != -1:
            self.requests.move_to_end(best_id)

        return best_len, best_id

    def add_request(self, request_id: int, tokens: List[int]) -> None:
        """Add a request to the pool, evicting LRU entries if necessary."""
        # Evict until we have space
        while self.current_tokens + len(tokens) > self.max_tokens and self.requests:
            old_id, old_tokens = self.requests.popitem(last=False)
            self.current_tokens -= len(old_tokens)

        # Add new request
        self.requests[request_id] = tokens
        self.current_tokens += len(tokens)


def load_and_tokenize_inputs(
    jsonl_path: str, tokenizer_name: str = DEFAULT_TOKENIZER
) -> List[List[int]]:
    print(f"Loading tokenizer: {tokenizer_name}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)

    print(f"Reading and tokenizing inputs from: {jsonl_path}")
    tokenized_sequences = []

    with open(jsonl_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in tqdm(lines, desc="Tokenizing"):
        try:
            data = json.loads(line.strip())
            input_text = data.get("input", "")
            tokens = tokenizer.encode(input_text)
            tokenized_sequences.append(tokens)
        except Exception as e:
            print(f"Warning: Failed to process line: {e}")
            tokenized_sequences.append([])

    return tokenized_sequences


def calculate_hit_rate(
    token_sequences: List[List[int]], pool_size: Optional[int] = None
) -> float:
    # Use float('inf') for unlimited case to avoid eviction
    max_tokens = float("inf") if pool_size is None else pool_size
    pool = LRUTokenPool(max_tokens)

    total_tokens = 0
    hit_tokens = 0

    for idx, tokens in enumerate(token_sequences):
        total_tokens += len(tokens)

        if idx > 0:
            common, _ = pool.longest_prefix_len(tokens)
            hit_tokens += common

        pool.add_request(idx, tokens)

    return hit_tokens / total_tokens if total_tokens > 0 else 0.0


def analyze_hit_rates_across_pool_sizes(
    token_sequences: List[List[int]],
    pool_sizes_gb: List[Union[int, float, str]],
    tokens_per_gb: int,
) -> Tuple[List[float], List[str]]:
    print("\nAnalyzing hit rates across pool sizes...")
    print("=" * 60)

    hit_rates = []
    x_labels = []

    for size_gb in pool_sizes_gb:
        if size_gb == "unlimited":
            size_tokens = None
            x_labels.append("∞")
            pool_desc = "unlimited"
            token_desc = ""
        else:
            size_tokens = int(size_gb * tokens_per_gb)
            x_labels.append(str(int(size_gb)))
            pool_desc = f"{size_gb}GB"
            token_desc = f" ({size_tokens:,} tokens)"

        print(f"Testing pool size: {pool_desc}{token_desc}")
        hit_rate = calculate_hit_rate(token_sequences, size_tokens)
        hit_rates.append(hit_rate)
        print(f"  Hit rate: {hit_rate:.4f} ({hit_rate * 100:.2f}%)\n")

    print("=" * 60)
    return hit_rates, x_labels


def plot_hit_rates(
    hit_rates: List[float], x_labels: List[str], output_path: str
) -> None:
    """
    Generate and save the hit rate vs pool size plot.

    Args:
        hit_rates: List of hit rates
        x_labels: X-axis labels (pool sizes)
        output_path: Path to save the plot
    """
    plt.figure(figsize=(12, 7))
    plt.plot(
        range(len(hit_rates)),
        hit_rates,
        marker="o",
        linewidth=2,
        markersize=8,
        color="#2E86AB",
    )

    plt.xlabel("Pool Size (GB)", fontsize=12, fontweight="bold")
    plt.ylabel("Hit Rate", fontsize=12, fontweight="bold")
    plt.title("Prefix Cache Hit Rate vs Pool Size", fontsize=14, fontweight="bold")
    plt.xticks(range(len(x_labels)), x_labels, rotation=45)
    plt.grid(True, alpha=0.3, linestyle="--")
    plt.ylim(0, min(1.0, max(hit_rates) * 1.1))

    for i, (rate, label) in enumerate(zip(hit_rates, x_labels, strict=False)):
        plt.annotate(
            f"{rate * 100:.1f}%",
            xy=(i, rate),
            xytext=(0, 8),
            textcoords="offset points",
            ha="center",
            fontsize=9,
            fontweight="bold",
        )

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches="tight")
    print(f"Plot saved to: {output_path}")


def parse_arguments() -> argparse.Namespace:
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Analyze prefix cache hit rates across different pool sizes",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
#   %(prog)s -i trace.jsonl
#   %(prog)s -i trace.jsonl -o custom_output.png
#   %(prog)s -i trace.jsonl --pool-sizes 1 2 4 8 16 unlimited
        """,
    )

    parser.add_argument(
        "-i",
        "--input",
        type=str,
        required=True,
        help="Path to input JSONL file (trace.jsonl)",
    )

    parser.add_argument(
        "-o",
        "--output",
        type=str,
        default="prefix_cache_hit_rate.png",
        help="Path to output plot file (PNG) (default: prefix_cache_hit_rate.png)",
    )

    parser.add_argument(
        "--tokenizer",
        type=str,
        default=DEFAULT_TOKENIZER,
        help=f"HuggingFace tokenizer model name (default: {DEFAULT_TOKENIZER})",
    )

    parser.add_argument(
        "--tokens-per-gb",
        type=int,
        default=DEFAULT_TOKENS_PER_GB,
        help=f"Conversion factor from GB to tokens "
        f"(default: {DEFAULT_TOKENS_PER_GB}). "
        "This should be adjusted when using a different tokenizer.",
    )

    parser.add_argument(
        "--pool-sizes",
        nargs="+",
        default=None,
        help='Pool sizes in GB to test (space-separated, can include "unlimited"). '
        f"Default: {' '.join(map(str, DEFAULT_POOL_SIZES_GB))}",
    )

    return parser.parse_args()


def parse_pool_sizes(
    pool_sizes_input: Optional[List[str]],
) -> List[Union[int, float, str]]:
    if pool_sizes_input is None:
        return DEFAULT_POOL_SIZES_GB

    parsed_sizes: List[Union[int, float, str]] = []
    for size in pool_sizes_input:
        if size.lower() == "unlimited":
            parsed_sizes.append("unlimited")
        else:
            try:
                parsed_sizes.append(float(size))
            except ValueError:
                raise ValueError(
                    f"Invalid pool size: {size}. Must be a number or 'unlimited'"
                ) from None

    return parsed_sizes


def main() -> None:
    args = parse_arguments()

    # Parse pool sizes
    pool_sizes_gb = parse_pool_sizes(args.pool_sizes)

    print("Configuration:")
    print(f"  Input: {args.input}")
    print(f"  Output: {args.output}")
    print(f"  Tokenizer: {args.tokenizer}")
    print(f"  Tokens per GB: {args.tokens_per_gb}")
    print(f"  Pool sizes: {pool_sizes_gb}\n")

    # Load and tokenize inputs
    token_sequences = load_and_tokenize_inputs(args.input, args.tokenizer)
    print(f"Loaded {len(token_sequences)} requests")

    # Analyze hit rates
    hit_rates, x_labels = analyze_hit_rates_across_pool_sizes(
        token_sequences, pool_sizes_gb, args.tokens_per_gb
    )

    # Generate plot
    plot_hit_rates(hit_rates, x_labels, args.output)
    print("\nAnalysis complete!")


if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/examples/kv_cache_calculator
!pip install -r requirement.txt

!python generate_config.py --model facebook/opt-125m

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/examples/kv_cache_reuse/local_backends
!python offload.py

!huggingface-cli login

# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: Copyright contributors to the vLLM project
"""
This file demonstrates the example usage of cpu offloading
with LMCache in vLLM v1 or v0.

Usage:

    Specify vLLM version

    -v v0 : Use LMCacheConnector
            model = mistralai/Mistral-7B-Instruct-v0.2
            (Includes enable_chunked_prefill = True)

    -v v1 : Use LMCacheConnectorV1 (default)
            model = meta-llama/Meta-Llama-3.1-8B-Instruct
            (Without enable_chunked_prefill)

Note that `lmcache` is needed to run this example.
Requirements:
https://docs.lmcache.ai/getting_started/installation.html#prerequisites
Learn more about LMCache environment setup, please refer to:
https://docs.lmcache.ai/getting_started/installation.html
"""

import argparse
import contextlib
import os
import time
from dataclasses import asdict

from lmcache.integration.vllm.utils import ENGINE_NAME
from lmcache.v1.cache_engine import LMCacheEngineBuilder

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig
from vllm.engine.arg_utils import EngineArgs


def setup_environment_variables(vllm_version: str):
    # LMCache-related environment variables
    # Use experimental features in LMCache
    os.environ["LMCACHE_USE_EXPERIMENTAL"] = "True"
    # LMCache is set to use 256 tokens per chunk
    os.environ["LMCACHE_CHUNK_SIZE"] = "256"
    # Enable local CPU backend in LMCache
    os.environ["LMCACHE_LOCAL_CPU"] = "True"
    # Set local CPU memory limit to 5.0 GB
    os.environ["LMCACHE_MAX_LOCAL_CPU_SIZE"] = "5.0"
    if vllm_version == "v0":
        os.environ["VLLM_USE_V1"] = "0"


@contextlib.contextmanager
def build_llm_with_lmcache(lmcache_connector: str, model: str, vllm_version: str):
    ktc = KVTransferConfig(
        kv_connector=lmcache_connector,
        kv_role="kv_both",
    )
    # Set GPU memory utilization to 0.8 for an A40 GPU with 40GB
    # memory. Reduce the value if your GPU has less memory.
    # Note: LMCache supports chunked prefill (see vLLM#14505, LMCache#392).
    if vllm_version == "v0":
        llm_args = EngineArgs(
            model=model,
            kv_transfer_config=ktc,
            max_model_len=8000,
            gpu_memory_utilization=0.8,
            enable_chunked_prefill=True,  # Only in v0
        )
    else:
        llm_args = EngineArgs(
            model=model,
            kv_transfer_config=ktc,
            max_model_len=8000,
            gpu_memory_utilization=0.8,
        )

    llm = LLM(**asdict(llm_args))
    try:
        yield llm
    finally:
        # Clean up lmcache backend
        LMCacheEngineBuilder.destroy(ENGINE_NAME)


def print_output(
    llm: LLM,
    prompt: list[str],
    sampling_params: SamplingParams,
    req_str: str,
):
    # Should be able to see logs like the following:
    # `LMCache INFO: Storing KV cache for 6006 out of 6006 tokens for request 0`
    # This indicates that the KV cache has been stored in LMCache.
    start = time.time()
    outputs = llm.generate(prompt, sampling_params)
    print("-" * 50)
    for output in outputs:
        generated_text = output.outputs[0].text
        print(f"Generated text: {generated_text!r}")
    print(f"Generation took {time.time() - start:.2f} seconds, {req_str} request done.")
    print("-" * 50)


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-v",
        "--version",
        choices=["v0", "v1"],
        default="v1",
        help="Specify vLLM version (default: v1)",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    if args.version == "v0":
        lmcache_connector = "LMCacheConnector"
        model = "mistralai/Mistral-7B-Instruct-v0.2"
    else:
        lmcache_connector = "LMCacheConnectorV1"
        model = "meta-llama/Meta-Llama-3.1-8B-Instruct"

    setup_environment_variables(args.version)

    with build_llm_with_lmcache(lmcache_connector, model, args.version) as llm:
        # This example script runs two requests with a shared prefix.
        # Define the shared prompt and specific prompts
        shared_prompt = "Hello, how are you?" * 1000
        first_prompt = [
            shared_prompt + "Hello, my name is",
        ]
        second_prompt = [
            shared_prompt + "Tell me a very long story",
        ]

        sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)

        # Print the first output
        print_output(llm, first_prompt, sampling_params, "first")

        time.sleep(1)

        # print the second output
        print_output(llm, second_prompt, sampling_params, "second")


if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/lmcache/v1
!python basic_check.py -h

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/lmcache/v1
!python basic_check.py --model facebook/opt-125m --mode lest

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/lmcache/v1
!python basic_check.py -h

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/lmcache/v1
!python basic_check.py --mode list

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache/lmcache/v1
!python basic_check.py --model facebook/opt-125m --mode gen

!pip show LMCache
!pip show vllm

import os

# Set token chunk size to 256
os.environ["LMCACHE_CHUNK_SIZE"] = "256"
# Enable CPU memory backend
os.environ["LMCACHE_LOCAL_CPU"] = "True"
# Set CPU memory limit to 5GB
os.environ["LMCACHE_MAX_LOCAL_CPU_SIZE"] = "5.0"

!!vllm serve "facebook/opt-125m" \
    --kv-transfer-config \
    '{"kv_connector":"LMCacheConnectorV1Dynamic","kv_role":"kv_both","kv_connector_module_path":"lmcache.integration.vllm.lmcache_connector_v1"}'

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# Configure KV cache transfer to use LMCache
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

# Initialize LLM with LMCache configuration
# Adjust gpu_memory_utilization based on your GPU memory
llm = LLM(model="Qwen/Qwen3-8B",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# Configure KV cache transfer to use LMCache
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

# Initialize LLM with LMCache configuration
# Adjust gpu_memory_utilization based on your GPU memory
llm = LLM(model="Qwen/Qwen3-8B",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)



# Create example prompts with shared prefix
shared_prompt = "Hello, how are you?" * 1000
prompts = [
    shared_prompt + "Hello, my name is",
]

# Define sampling parameters
sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)

# Run inference
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    generated_text = output.outputs[0].text
    print(f"Generated text: {generated_text!r}")

from lmcache.v1.cache_engine import LMCacheEngineBuilder
from lmcache.integration.vllm.utils import ENGINE_NAME

LMCacheEngineBuilder.destroy(ENGINE_NAME)

!pip uninstall -y vllm lmcache

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache

!pip install -r requirements/build.txt

!pip install -e .

!!pip install vllm

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache
!python  lmcache/cpu-offloading.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/LMCache

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# يجب أن يعمل هذا الكود الآن
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

llm = LLM(model="Qwen/Qwen3-8B",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)

print("LLM with LMCache initialized successfully!")

/content/LMCache
INFO 10-25 21:52:54 [__init__.py:216] Automatically detected platform cuda.
INFO 10-25 21:53:06 [utils.py:233] non-default args: {'max_model_len': 8000, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='2f802c38-fc9c-483c-9001-49433d79418a', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None), 'model': 'Qwen/Qwen3-8B'}
/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning:
The secret `HF_TOKEN` does not exist in your Colab secrets.
To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.
You will be able to reuse this secret in all of your notebooks.
Please note that authentication is recommended but still optional to access public models or datasets.
  warnings.warn(
INFO 10-25 21:53:07 [model.py:547] Resolved architecture: Qwen3ForCausalLM
`torch_dtype` is deprecated! Use `dtype` instead!
WARNING 10-25 21:53:07 [model.py:1682] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 10-25 21:53:07 [model.py:1733] Casting torch.bfloat16 to torch.float16.
INFO 10-25 21:53:07 [model.py:1510] Using max model len 8000
INFO 10-25 21:53:11 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.
WARNING 10-25 21:53:12 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# Configure KV cache transfer to use LMCache
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

# Initialize LLM with LMCache configuration
# Adjust gpu_memory_utilization based on your GPU memory
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)



# Create example prompts with shared prefix
shared_prompt = "Hello, how are you?" * 1000
prompts = [
    shared_prompt + "Hello, my name is",
]

# Define sampling parameters
sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=10)

# Run inference
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    generated_text = output.outputs[0].text
    print(f"Generated text: {generated_text!r}")



from PIL import Image
from IPython.display import display

# Replace 'path/to/your/image.jpg' with the actual path to your image file
image_path = '/content/lmcache.png'

try:
    # Open the image file
    img = Image.open(image_path)

    # Display the image in the notebook output
    display(img)

except FileNotFoundError:
    print(f"Error: Image file not found at '{image_path}'")
except Exception as e:
    print(f"An error occurred: {e}")

from PIL import Image
from IPython.display import display

# Replace 'path/to/your/image.jpg' with the actual path to your image file
image_path = '/content/1.png'

try:
    # Open the image file
    img = Image.open(image_path)

    # Display the image in the notebook output
    display(img)

except FileNotFoundError:
    print(f"Error: Image file not found at '{image_path}'")
except Exception as e:
    print(f"An error occurred: {e}")

from vllm import LLM, SamplingParams
from vllm.config import KVTransferConfig

# Configure KV cache transfer to use LMCache
ktc = KVTransferConfig(
    kv_connector="LMCacheConnectorV1",
    kv_role="kv_both",
)

# Initialize LLM with LMCache configuration
# Adjust gpu_memory_utilization based on your GPU memory
llm = LLM(model="meta-llama/Llama-3.2-1B-Instruct",
          kv_transfer_config=ktc,
          max_model_len=8000,
          gpu_memory_utilization=0.8)



# Create example prompts with shared prefix
shared_prompt = "Hello, how are you?" * 1000
prompts = [
    shared_prompt + "python is",
]

# Define sampling parameters
sampling_params = SamplingParams(temperature=0, top_p=0.95, max_tokens=128)

# Run inference
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    generated_text = output.outputs[0].text
    print(f"Generated text: {generated_text!r}")

